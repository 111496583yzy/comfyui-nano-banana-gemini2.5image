"""
Gemini å›¾åƒç¼–è¾‘èŠ‚ç‚¹
æ”¯æŒå•å›¾å’Œå¤šå›¾è¾“å…¥ï¼Œè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡æ•°æ®
"""

import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import time
import random
from typing import Optional, Tuple, Dict, Any, List

try:
    from .tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    from .utils import (
        image_to_base64, base64_to_image,
        validate_api_key, format_error_message, resize_image_for_api
    )
    from .config import DEFAULT_CONFIG
except ImportError:
    from .tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    # Fallback utility functions - å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨å†…ç½®ç‰ˆæœ¬
    pass
    
    def image_to_base64(image, format='JPEG'):
        buffer = io.BytesIO()
        if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        image.save(buffer, format=format, quality=95)
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    
    def validate_api_key(api_key):
        return api_key and len(api_key.strip()) > 10
    
    def format_error_message(error):
        return str(error)
    
    DEFAULT_CONFIG = {"timeout": 120, "max_retries": 3}


def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ"""
    base_delay = 2 ** attempt
    
    if error_code == 429:
        rate_limit_delay = 60 + random.uniform(10, 30)
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:
        return base_delay + random.uniform(1, 5)
    else:
        return base_delay


class GeminiImageEdit:
    """Gemini å›¾åƒç¼–è¾‘èŠ‚ç‚¹ - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†æ–¹å¼é¿å…ç™½è¾¹é—®é¢˜"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "Describe these images and edit them", "multiline": True}),
                "model": (["gemini-2.5-flash-image", "gemini-2.0-flash-preview-image-generation"], {"default": "gemini-2.5-flash-image"}),
                "aspectRatio": ([
                    "auto",     # è‡ªåŠ¨é€‰æ‹©æœ€ä½³é•¿å®½æ¯”
                    "1:1",      # æ­£æ–¹å½¢
                    "9:16",     # ç«–å±
                    "16:9",     # æ¨ªå±
                    "3:4",      # ç«–å±
                    "4:3",      # æ¨ªå±
                    "3:2",      # æ¨ªå±
                    "2:3",      # ç«–å±
                    "5:4",      # æ¨ªå±
                    "4:5",      # ç«–å±
                    "21:9",     # è¶…å®½å±
                ], {"default": "auto"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.05}),
                "max_output_tokens": ("INT", {"default": 8192, "min": 1, "max": 32768}),
            },
            "optional": {
                "images": ("IMAGE",),
                "image_1": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "image_4": ("IMAGE",),
                "image_5": ("IMAGE",),
                "image_6": ("IMAGE",),
                "system_instruction": ("STRING", {"default": "", "multiline": True, "placeholder": "å¯é€‰ï¼šç³»ç»Ÿæç¤ºè¯ï¼Œä¸ºç©ºæ—¶ä¸å‘é€"}),
            }
        }
        
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "process_images"
    CATEGORY = "Nano"

    def process_images(self, api_key, prompt, model, aspectRatio="auto", seed=0, temperature=1.0, top_p=0.95, max_output_tokens=8192, system_instruction="", 
                      images=None, image_1=None, image_2=None, image_3=None, image_4=None, image_5=None, image_6=None):
        """å¤„ç†å›¾åƒå¹¶è¿”å›ç¼–è¾‘åçš„å›¾åƒå’Œå“åº”æ–‡æœ¬"""
        
        # æ£€æŸ¥APIå¯†é’¥
        if not api_key:
            raise ValueError("è¯·æä¾›æœ‰æ•ˆçš„Gemini APIå¯†é’¥")
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒ
        pil_images = []
        
        # å¤„ç†æ‰¹æ¬¡å›¾åƒï¼ˆå‘åå…¼å®¹ï¼‰
        if images is not None:
            batch_images = [tensor_to_pil(images[i]) for i in range(images.shape[0])]
            pil_images.extend(batch_images)
            print(f"ğŸ“¥ ä»æ‰¹æ¬¡å›¾åƒæ”¶åˆ° {len(batch_images)} å¼ å›¾åƒ")
        
        # å¤„ç†6ä¸ªç‹¬ç«‹çš„å›¾åƒè¾“å…¥
        individual_images = [image_1, image_2, image_3, image_4, image_5, image_6]
        image_names = ["image_1", "image_2", "image_3", "image_4", "image_5", "image_6"]
        
        for i, img in enumerate(individual_images):
            if img is not None:
                pil_image = tensor_to_pil(img)
                pil_images.append(pil_image)
                print(f"ğŸ“¥ æ”¶åˆ° {image_names[i]}: {pil_image.size}")
        
        if not pil_images:
            raise ValueError("è¯·è‡³å°‘æä¾›ä¸€å¼ å›¾åƒ")
        
        print(f"ğŸ“¥ æ€»å…±æ”¶åˆ° {len(pil_images)} å¼ å›¾åƒè¿›è¡Œå¤„ç†")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†æ¨¡å¼å¤„ç†å›¾åƒ
        print(f"ğŸ”„ ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†æ¨¡å¼å¤„ç†å›¾åƒ")
        print(f"â„¹ï¸ Received seed {seed}, but the Gemini API does not currently support a seed parameter for image editing.")
        print(f"ğŸ“ ä½¿ç”¨é•¿å®½æ¯”: {aspectRatio}")
        edited_tensor, response_text = self._process_combined_images(api_key, pil_images, prompt, model, aspectRatio, temperature, top_p, max_output_tokens, system_instruction)
        
        return (edited_tensor, response_text)
    
    def _process_combined_images(self, api_key: str, pil_images: List[Image.Image], prompt: str, model: str, aspectRatio: str,                                                                                         
                                temperature: float, top_p: float, max_output_tokens: int, system_instruction: str = "") -> Tuple[torch.Tensor, str]:
        """å¤„ç†å¤šå¼ å›¾åƒï¼ˆåˆå¹¶å‘é€ï¼‰"""
        
        # æ„å»ºåŒ…å«å¤šå¼ å›¾åƒçš„è¯·æ±‚
        parts = [{"text": prompt.strip()}]
        
        # æ·»åŠ æ‰€æœ‰å›¾åƒ
        for i, pil_image in enumerate(pil_images):
            image_base64 = image_to_base64(pil_image, format='JPEG')
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            })
            print(f"ğŸ“ æ·»åŠ ç¬¬ {i+1} å¼ å›¾åƒåˆ°è¯·æ±‚ä¸­")
        
        # æ„å»ºAPI URL
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        
        # æ„å»ºè¯·æ±‚æ•°æ® - æ›´æ–°ä¸ºåŒ¹é…å®˜æ–¹ç¤ºä¾‹çš„æ ¼å¼
        generation_config = {
            "temperature": temperature,
            "topP": top_p,
            "maxOutputTokens": max_output_tokens,
            "responseModalities": ["IMAGE", "TEXT"]
        }
        
        # åªæœ‰å½“é•¿å®½æ¯”ä¸æ˜¯ "auto" æ—¶æ‰æ·»åŠ  imageConfig åˆ° generationConfig
        if aspectRatio != "auto":
            generation_config["imageConfig"] = {
                "aspectRatio": aspectRatio
            }
        
        request_data = {
            "contents": [{
                "parts": parts
            }],
            "generationConfig": generation_config
        }
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼‰
        if system_instruction and system_instruction.strip():
            request_data["systemInstruction"] = {
                "parts": [{"text": system_instruction.strip()}]
            }
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key.strip()
        }
        
        # å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
        return self._send_request_and_process(url, headers, request_data, pil_images[0], model)
    
    def _send_request_and_process(self, url: str, headers: dict, request_data: dict, 
                                 fallback_image: Image.Image, model: str) -> Tuple[torch.Tensor, str]:
        """å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”"""
        
        max_retries = 5
        timeout = DEFAULT_CONFIG.get("timeout", 120)
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨å¤„ç†å›¾åƒ... (å°è¯• {attempt + 1}/{max_retries}) ä½¿ç”¨æ¨¡å‹: {model}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(url, headers=headers, json=request_data, timeout=timeout)
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    result = response.json()
                    print(f"ğŸ“‹ APIå“åº”ç»“æ„: {list(result.keys())}")
                    
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    if "candidates" in result and result["candidates"]:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            for part in candidate["content"]["parts"]:
                                # æå–æ–‡æœ¬
                                if "text" in part:
                                    response_text += part["text"]
                                
                                # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                if "inline_data" in part or "inlineData" in part:
                                    inline_data = part.get("inline_data") or part.get("inlineData")
                                    if inline_data and "data" in inline_data:
                                        try:
                                            image_data = inline_data["data"]
                                            image_bytes = base64.b64decode(image_data)
                                            edited_image = Image.open(io.BytesIO(image_bytes))
                                            print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                        except Exception as e:
                                            print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = fallback_image
                        if not response_text:
                            response_text = "å›¾ç‰‡å¤„ç†è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å›¾ç‰‡å¤„ç†å®Œæˆ")
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {error_msg}")


class GeminiImageGenerate:
    """Gemini å›¾åƒç”ŸæˆèŠ‚ç‚¹ - æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme", "multiline": True}),
                "model": (["gemini-2.5-flash-image", "gemini-2.0-flash-preview-image-generation"], {"default": "gemini-2.5-flash-image"}),
                "aspectRatio": ([
                    "auto",     # è‡ªåŠ¨é€‰æ‹©æœ€ä½³é•¿å®½æ¯”
                    "1:1",      # æ­£æ–¹å½¢
                    "9:16",     # ç«–å±
                    "16:9",     # æ¨ªå±
                    "3:4",      # ç«–å±
                    "4:3",      # æ¨ªå±
                    "3:2",      # æ¨ªå±
                    "2:3",      # ç«–å±
                    "5:4",      # æ¨ªå±
                    "4:5",      # ç«–å±
                    "21:9",     # è¶…å®½å±
                ], {"default": "auto"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.05}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 1, "max": 8192}),
            },
            "optional": {
                "system_instruction": ("STRING", {"default": "", "multiline": True, "placeholder": "å¯é€‰ï¼šç³»ç»Ÿæç¤ºè¯ï¼Œä¸ºç©ºæ—¶ä¸å‘é€"}),
            }
        }
        
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("generated_image", "response_text")
    FUNCTION = "generate_images"
    CATEGORY = "Nano"

    def generate_images(self, api_key: str, prompt: str, model: str, aspectRatio: str, seed: int,
                        temperature: float, top_p: float, max_output_tokens: int, system_instruction: str = "") -> Tuple[torch.Tensor, str]:
        
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")

        print(f"â„¹ï¸ Received seed {seed}, but the Gemini API does not currently support a seed parameter for image generation.")
        print(f"ğŸ“ ä½¿ç”¨é•¿å®½æ¯”: {aspectRatio}")

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        
        generation_config = {
            "candidateCount": 1,
            "temperature": temperature,
            "topP": top_p,
            "maxOutputTokens": max_output_tokens,
            "responseModalities": ["IMAGE", "TEXT"]
        }
        
        # åªæœ‰å½“é•¿å®½æ¯”ä¸æ˜¯ "auto" æ—¶æ‰æ·»åŠ  imageConfig åˆ° generationConfig
        if aspectRatio != "auto":
            generation_config["imageConfig"] = {
                "aspectRatio": aspectRatio
            }
        
        request_data = {
            "contents": [{
                "parts": [
                    {"text": prompt.strip()}
                ]
            }],
            "generationConfig": generation_config
        }
        
        # åªæœ‰å½“ç³»ç»Ÿæç¤ºè¯ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ  systemInstruction
        if system_instruction and system_instruction.strip():
            request_data["systemInstruction"] = {
                "parts": [
                    {"text": system_instruction.strip()}
                ]
            }
        
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key.strip()
        }
        
        return self._send_request_and_generate_images(url, headers, request_data, model)

    def _send_request_and_generate_images(self, url: str, headers: dict, request_data: dict, model: str) -> Tuple[torch.Tensor, str]:
        """å‘é€ç”Ÿæˆè¯·æ±‚å¹¶å¤„ç†å“åº”"""
        
        max_retries = 5
        timeout = DEFAULT_CONFIG.get("timeout", 120)
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ... (å°è¯• {attempt + 1}/{max_retries}) ä½¿ç”¨æ¨¡å‹: {model}")
                
                response = requests.post(url, headers=headers, json=request_data, timeout=timeout)
                
                if response.status_code == 200:
                    result = response.json()
                    generated_images = []
                    response_texts = []
                    
                    if "candidates" in result and result["candidates"]:
                        for i, candidate in enumerate(result["candidates"]):
                            candidate_text = ""
                            candidate_image = None
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    if "text" in part:
                                        candidate_text += part["text"]
                                    
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                candidate_image = Image.open(io.BytesIO(image_bytes))
                                            except Exception as e:
                                                print(f"âš ï¸ è§£ç å€™é€‰å›¾ç‰‡ {i+1} å¤±è´¥: {e}")
                            
                            if candidate_image:
                                generated_images.append(candidate_image)
                                response_texts.append(f"å›¾åƒ {i+1}:\n{candidate_text}")
                                print(f"âœ… æˆåŠŸæå–ç”Ÿæˆçš„å›¾ç‰‡ {i+1}")

                    if not generated_images:
                        raise ValueError("APIå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç”Ÿæˆå›¾ç‰‡")
                    
                    if len(generated_images) == 1:
                        final_tensor = pil_to_tensor(generated_images[0])
                    else:
                        tensors = [pil_to_tensor(img) for img in generated_images]
                        final_tensor = torch.stack(tensors, dim=0)
                    
                    combined_response = "\n\n".join(response_texts)
                    print(f"âœ… å›¾åƒç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºå¼ é‡å½¢çŠ¶: {final_tensor.shape}")
                    return (final_tensor, combined_response)
                
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
            
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾åƒç”Ÿæˆå¤±è´¥: {error_msg}")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "GeminiImageEdit": GeminiImageEdit,
    "GeminiImageGenerate": GeminiImageGenerate,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiImageEdit": "Gemini å›¾åƒç¼–è¾‘",
    "GeminiImageGenerate": "Gemini å›¾åƒç”Ÿæˆ",
}